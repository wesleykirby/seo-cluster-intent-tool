"""Utilities for routing keywords into high level verticals.

This module provides a :class:`VerticalRouter` classifier that combines
vector embeddings from a SentenceTransformer model with handcrafted rule-hit
features.  The router is trained with weak supervision generated by the same
rules, and can be persisted to disk for use inside the clustering pipeline.
"""
from __future__ import annotations

from dataclasses import dataclass, field
from pathlib import Path
from typing import Iterable, List, Optional, Sequence, Tuple
import re
import numpy as np
import joblib


@dataclass
class RouterRule:
    """Simple regex based rule used both for weak labels and features."""

    name: str
    pattern: str
    label: str
    weight: float = 1.0
    _compiled: re.Pattern = field(init=False, repr=False)

    def __post_init__(self) -> None:
        self._compiled = re.compile(self.pattern, re.IGNORECASE)

    def score(self, text: str) -> float:
        """Return the rule weight if the regex matches the text."""
        return self.weight if self._compiled.search(text) else 0.0

    def __getstate__(self):  # pragma: no cover - invoked by joblib
        state = self.__dict__.copy()
        state.pop("_compiled", None)
        return state

    def __setstate__(self, state):  # pragma: no cover - invoked by joblib
        self.__dict__.update(state)
        self._compiled = re.compile(self.pattern, re.IGNORECASE)


DEFAULT_VERTICAL_RULES: List[RouterRule] = [
    RouterRule(
        name="sports_terms",
        pattern=r"\b(bet|betting|odds|fixture|fixtures|match|matches|soccer|football|sport|sportsbook|handicap|accumulator)\b",
        label="sportsbook",
        weight=1.2,
    ),
    RouterRule(
        name="live_betting",
        pattern=r"\b(in-play|inplay|live betting|live odds|cashout|cash out)\b",
        label="sportsbook",
        weight=1.0,
    ),
    RouterRule(
        name="casino_terms",
        pattern=r"\b(casino|slot|slots|roulette|blackjack|baccarat|craps|spin|megaways)\b",
        label="casino",
        weight=1.3,
    ),
    RouterRule(
        name="poker_terms",
        pattern=r"\b(poker|texas hold\s*'?(em)?|holdem|omaha)\b",
        label="poker",
        weight=1.0,
    ),
    RouterRule(
        name="lottery_terms",
        pattern=r"\b(lottery|lotto|jackpot|jack pot|powerball|mega millions|raffle)\b",
        label="lottery",
        weight=1.1,
    ),
    RouterRule(
        name="payments",
        pattern=r"\b(deposit|withdraw|withdrawal|mpesa|airtel money|payment|cashout|paybill)\b",
        label="payments",
        weight=0.9,
    ),
]


def build_rule_feature_matrix(texts: Sequence[str], rules: Optional[Sequence[RouterRule]] = None) -> np.ndarray:
    """Return a dense matrix with rule-hit scores for the provided texts."""
    rules = list(rules) if rules is not None else DEFAULT_VERTICAL_RULES
    matrix = np.zeros((len(texts), len(rules)), dtype=np.float32)
    for i, text in enumerate(texts):
        for j, rule in enumerate(rules):
            matrix[i, j] = rule.score(text)
    return matrix


def weak_rule_labels(
    texts: Sequence[str], rules: Optional[Sequence[RouterRule]] = None
) -> Tuple[List[Optional[str]], List[float], List[float]]:
    """Generate weak labels and confidences using the supplied rules.

    Returns
    -------
    labels : list
        Chosen label per text (``None`` if no rule matches).
    confidences : list of float
        Ratio between the winning label score and the total rule score.
    strengths : list of float
        Raw score accumulated for the winning label.
    """

    rules = list(rules) if rules is not None else DEFAULT_VERTICAL_RULES
    labels: List[Optional[str]] = []
    confidences: List[float] = []
    strengths: List[float] = []

    for text in texts:
        label_scores: dict[str, float] = {}
        total = 0.0
        for rule in rules:
            s = rule.score(text)
            if s:
                total += s
                label_scores[rule.label] = label_scores.get(rule.label, 0.0) + s
        if not label_scores:
            labels.append(None)
            confidences.append(0.0)
            strengths.append(0.0)
            continue
        best_label, best_score = max(label_scores.items(), key=lambda kv: kv[1])
        conf = best_score / total if total > 0 else 0.0
        labels.append(best_label)
        confidences.append(conf)
        strengths.append(best_score)
    return labels, confidences, strengths


class VerticalRouter:
    """Classifier that combines embeddings and rule features for verticals."""

    def __init__(
        self,
        model,
        label_encoder,
        rules: Optional[Sequence[RouterRule]] = None,
        *,
        embedder_name: str = "all-MiniLM-L6-v2",
        min_confidence: float = 0.0,
        fallback_label: str = "general",
        normalize_embeddings: bool = True,
        embedding_batch_size: Optional[int] = None,
    ) -> None:
        self.model = model
        self.label_encoder = label_encoder
        self.rules = list(rules) if rules is not None else DEFAULT_VERTICAL_RULES
        self.embedder_name = embedder_name
        self.min_confidence = float(min_confidence)
        self.fallback_label = fallback_label
        self.normalize_embeddings = normalize_embeddings
        self.embedding_batch_size = embedding_batch_size
        self._embedder = None

    # ------------------------------------------------------------------
    # Persistence helpers
    def save(self, path: Path | str) -> None:
        data = {
            "model": self.model,
            "label_encoder": self.label_encoder,
            "rules": self.rules,
            "embedder_name": self.embedder_name,
            "min_confidence": self.min_confidence,
            "fallback_label": self.fallback_label,
            "normalize_embeddings": self.normalize_embeddings,
            "embedding_batch_size": self.embedding_batch_size,
        }
        joblib.dump(data, path)

    @classmethod
    def load(cls, path: Path | str) -> "VerticalRouter":
        data = joblib.load(path)
        return cls(**data)

    # ------------------------------------------------------------------
    def _get_embedder(self):  # pragma: no cover - heavy dependency
        if self._embedder is None:
            try:
                from sentence_transformers import SentenceTransformer
            except ImportError as exc:  # pragma: no cover - import guard
                raise RuntimeError(
                    "sentence-transformers must be installed to use VerticalRouter"
                ) from exc
            self._embedder = SentenceTransformer(self.embedder_name)
        return self._embedder

    def _prepare_texts(self, texts: Iterable[str]) -> List[str]:
        return [str(t) for t in texts]

    def _embed(self, texts: Sequence[str]) -> np.ndarray:
        embedder = self._get_embedder()
        embeddings = embedder.encode(
            texts,
            batch_size=self.embedding_batch_size,
            show_progress_bar=False,
            convert_to_numpy=True,
        )
        if self.normalize_embeddings:
            norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
            norms = np.clip(norms, 1e-12, None)
            embeddings = embeddings / norms
        return embeddings.astype(np.float32)

    def _build_features(self, texts: Sequence[str]) -> np.ndarray:
        embeddings = self._embed(texts)
        rule_feats = build_rule_feature_matrix(texts, self.rules)
        if rule_feats.size == 0:
            return embeddings
        return np.hstack([embeddings, rule_feats])

    def predict_proba(self, texts: Sequence[str]) -> np.ndarray:
        texts_list = self._prepare_texts(texts)
        feats = self._build_features(texts_list)
        return self.model.predict_proba(feats)

    def predict_with_confidence(
        self, texts: Sequence[str]
    ) -> Tuple[List[str], List[float]]:
        texts_list = self._prepare_texts(texts)
        probs = self.predict_proba(texts_list)
        if probs.ndim != 2 or probs.shape[1] == 0:
            raise ValueError("Model returned invalid probability array")
        max_idx = np.argmax(probs, axis=1)
        best_class = self.model.classes_[max_idx]
        labels = self.label_encoder.inverse_transform(best_class)
        confidences = probs[np.arange(len(texts_list)), max_idx]
        final_labels: List[str] = []
        for label, conf in zip(labels, confidences):
            if conf < self.min_confidence:
                final_labels.append(self.fallback_label)
            else:
                final_labels.append(label)
        return final_labels, confidences.tolist()

    def predict(self, texts: Sequence[str]) -> List[str]:
        labels, _ = self.predict_with_confidence(texts)
        return labels


__all__ = [
    "RouterRule",
    "DEFAULT_VERTICAL_RULES",
    "VerticalRouter",
    "build_rule_feature_matrix",
    "weak_rule_labels",
]
